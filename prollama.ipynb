{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json, os\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', default=None, type=str,help=\"The local path of the model. If None, the model will be downloaded from HuggingFace\")\n",
    "parser.add_argument('--interactive', action='store_true',help=\"If True, you can input instructions interactively. If False, the input instructions should be in the input_file.\")\n",
    "parser.add_argument('--input_file', default=None, help=\"You can put all your input instructions in this file (one instruction per line).\")\n",
    "parser.add_argument('--output_file', default=None, help=\"All the outputs will be saved in this file.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if args.interactive and args.input_file:\n",
    "        raise ValueError(\"interactive is True, but input_file is not None.\")\n",
    "    if (not args.interactive) and (args.input_file is None):\n",
    "        raise ValueError(\"interactive is False, but input_file is None.\")\n",
    "    if args.input_file and (args.output_file is None):\n",
    "        raise ValueError(\"input_file is not None, but output_file is None.\")\n",
    "\n",
    "    load_type = torch.bfloat16\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(0)\n",
    "    else:\n",
    "        raise ValueError(\"No GPU available.\")\n",
    "\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        args.model,\n",
    "        torch_dtype=load_type,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map='auto',\n",
    "        quantization_config=None\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(args.model)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if args.interactive:\n",
    "            while True:\n",
    "                raw_input_text = input(\"Input:\")\n",
    "                if len(raw_input_text.strip())==0:\n",
    "                    break\n",
    "                input_text = raw_input_text\n",
    "                input_text = tokenizer(input_text,return_tensors=\"pt\")  \n",
    "\n",
    "                generation_output = model.generate(\n",
    "                                input_ids = input_text[\"input_ids\"].to(device),\n",
    "                                attention_mask = input_text['attention_mask'].to(device),\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                generation_config = generation_config,\n",
    "                                output_attentions=False\n",
    "                            )\n",
    "                s = generation_output[0]\n",
    "                output = tokenizer.decode(s,skip_special_tokens=True)\n",
    "                print(\"Output:\",output)\n",
    "                print(\"\\n\")\n",
    "        else:\n",
    "            outputs=[]\n",
    "            with open(args.input_file, 'r') as f:\n",
    "                examples =f.read().splitlines()\n",
    "            print(\"Start generating...\")\n",
    "            for index, example in tqdm(enumerate(examples),total=len(examples)):\n",
    "                input_text = tokenizer(example,return_tensors=\"pt\")  #add_special_tokens=False ?\n",
    "\n",
    "                generation_output = model.generate(\n",
    "                    input_ids = input_text[\"input_ids\"].to(device),\n",
    "                    attention_mask = input_text['attention_mask'].to(device),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    generation_config = generation_config\n",
    "                )\n",
    "                s = generation_output[0]\n",
    "                output = tokenizer.decode(s,skip_special_tokens=True)\n",
    "                outputs.append(output)\n",
    "            with open(args.output_file,'w') as f:\n",
    "                f.write(\"\\n\".join(outputs))\n",
    "            print(\"All the outputs have been saved in\",args.output_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biotorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
